# -*- coding: utf-8 -*-
"""A319 Project 1 - Notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C3fPTBjtnZMvQRGbF8CX1nQUODNjigmb

# Data Loading
Untuk dataset dapat diunduh [di sini](https://www.kaggle.com/sid321axn/gold-price-prediction-dataset)
<br>Pada cell pertama kita akan import semua library yang akan diperlukan.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
import os
import zipfile
from scipy.stats import zscore
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.model_selection import GridSearchCV

url = 'https://query1.finance.yahoo.com/v7/finance/download/GOLD.AX?period1=1262304000&period2=1641686400&interval=1d&events=history&includeAdjustedClose=true'
df = pd.read_csv(url)
df

"""Dataset yang diatas adalah dataset harga emas rentan waktu 2010 sampai 2022, yang berarti lebih banyak memiliki data-data tahun lama. Hal ini menyebabkan error yang jauh ketika di test dengan data tahun 2022. Maka salah satu solusinya adalah mengganti dataset dengan time interval lebih sedikit. Dataset dibawah ini menggunakan harga emas rentan waktu 2019 sampai 2022, dan dapat diunduh pada link berikut: [YahooFinance](https://finance.yahoo.com/quote/GOLD.AX/history?period1=1546300800&period2=1641772800&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true)"""

url = 'https://query1.finance.yahoo.com/v7/finance/download/GOLD.AX?period1=1546300800&period2=1641772800&interval=1d&events=history&includeAdjustedClose=true'
df = pd.read_csv(url)
df

date = df['Date'].values
close  = df['Adj Close'].values

plt.plot(date, close)
plt.title('Gold Price 2019-2022', fontsize=20);

print(df['Adj Close'][0])
print(df['Adj Close'][200])
print(df['Adj Close'][400])
print(df['Adj Close'][689])  # df - 10%
print(df['Adj Close'][650])  # df - 15%
print(df['Adj Close'][612])  # df - 20%
print(df['Adj Close'][700])
print(df['Adj Close'][766])

df.describe()

close_median = df['Adj Close'].median()
vol_median = df['Volume'].median()
open_median = df['Open'].median()
high_median = df['High'].median()
low_median = df['Low'].median()

print(close_median)
print(vol_median)
print(open_median)
print(high_median)
print(low_median)

"""# Mendeteksi Missing Value"""

df.info()

"""Jika kita lihat pada output diatas dataset kita tidak memiliki niai Null atau tidak ada missing value. Maka kita akan lanjut mendeteksi outliers dan menggantinya.

# Menangani Outliers
Untuk mendeteksi outliers kita bisa menggunakan beberapa teknik, antara lain:
* Hypothesis Testing
* Z-score method
* IQR Method

Sebelumnya, untuk mengetahui outliers kita bisa melakukan teknik visualisasi Boxplot. Maka dari itu kita akan melakukan visualisasi terlebih dahulu.
"""

sns.boxplot(x=df['Adj Close'])

sns.boxplot(x=df['Volume'])

"""Dari visualasi boxplot diatas kita bisa lihat bahwa dataset kita memiliki outliers, terutama pada column yang kita akan pakai yaitu column 'Adj Close', dan 'Volume'. Kita bisa saja menghapusnya tetapi karena ini merupakan dataset harian maka akan menghasilkan data yang hilang. Oleh karena itu kita akan menggantinya dengan nilai Median."""

df['Adj Close'].hist()

df['Volume'].hist()

df['Open'].hist()

df['High'].hist()

df['Low'].hist()

"""Jika kita lihat pada [03] pada bagian histogram ia menyatakan jika data pada histogram terdistribusi ke arah kiri itu menunjukan adanya outliers pada data. Pada histogram volume kita, kita bisa lihat bahwa data terdistribusi ke arah kiri yang mengindikasikan adanya outliers pada data.

Dikutip dari [03] tahap untuk memastikan adanya outliers adalah dengan cara melihat skewness value. Skewness dari rentan -1 sampai 1 terbilang normal distribution, dan nilai yang perubahannya sangat besar mengindikasikan adanya outliers.
"""

print('skewness value of Adj Close: ',df['Adj Close'].skew())
print('skewness value of Volume: ',df['Volume'].skew())
print('skewness value of Open: ',df['Open'].skew())
print('skewness value of High: ',df['High'].skew())
print('skewness value of Low: ',df['Low'].skew())

"""Dari output diatas kita bisa lihat bahwa data pada column Volume memiliki outliers, karena memiliki nilai 4.8 yang berarti rightly skewed yang mengindikasikan adanya outliers.

Selanjutnya kita akan menggantikan outliers dengan nilai Median pada data.Dikutip dari [03] tidak di rekomendasikan untuk menggantikannya dengan nilai Mean karena sangat rentan terhadap outlier. Ada beberapa teknik untuk menangani outliers, antara lain:
* Hypothesis Testing
* Z-score method
* IQR Method

Disini saya memilih untuk menggunakan IQR. Alasannya adalah saya lebih sering menggunakan metode IQR dan juga dari metode yang digunakan [03] adalah metode IQR. Dua cell dibawah akan mengeluarkan output outliers.
"""

Q1 = df['Volume'].quantile(0.25)
Q3 = df['Volume'].quantile(0.75)
IQR = Q3 - Q1
whisker_width = 1.5

vol_outliers = df[(df['Volume'] < Q1 - whisker_width*IQR) | (df['Volume'] > Q3 + whisker_width*IQR)]
vol_values=pd.DataFrame(vol_outliers['Volume'])
vol_values = vol_values.values
vol_median = df['Volume'].median()

df['Volume'] = df['Volume'].replace(to_replace=vol_values, value=vol_median)
df

"""Setelah kita menangani outliers kita dapat melanjutkan ke tahap Data Analysis menggunakan Multivariate Analysis."""

sns.pairplot(df, diag_kind = 'kde')

"""Dari grafik diatas kita bisa lihat bahwa column 'Open', 'High', dan 'Low' memiliki korelasi yang tinggi dengan output variabel kita yaitu 'Adj Close'."""

plt.figure(figsize=(10, 8))
correlation_matrix = df.corr().round(2)
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title('Correlation Matrix untuk Fitur Numerik', size=20)

"""Jika kita perhatikan column 'Open', 'High', dan 'Low' memiliki korelasi yang tinggi (diatas 0.9). Maka kita bisa drop column 'Volume', dan 'Close'. Alasannya adalah column Volume memiliki korelasi yang rendah sekali, dan column close tidak berguna karena outcome variabel kita adalah Adj Close . Perbedaan keduanya ada pada bagian deklarasi variabel.

Selanjutnya kita bisa lanjut ke tahap Data Preparation atau Pre-processing.

# Data Preparation
Karena dataset ini tidak memiliki fitur kategori maka kita tidak perlu melakukan encoding. Pada tahapan ini saya tidak melakukan reduksi dimensi dengan PCA dikarenakan saya ingin menjadi Open saja yang sebagai data input untuk memprediksi harga emas. Alasanya adalah karena jika kita ingin memprediksi harga emas kita belum tahu nilai 'High' dan 'Low'nya.

Sebelum masuk ke tahap Data Preparation kita harus melakukan train_test_split agar tidak terjadi kebocoran data. Pada cell pertama saya akan melakukan split. Jumlah test_size yang saya gunakan adalah 15% alasan saya menggunakan 15% adalah jika kita menggunakan 20% data test akan ada sebanyak 154 yang berarti terlalu banyak untuk dataset yang kecil seperti ini. Paramter shuffle saya jadikan False agar data tetap dalam urutan waktu yang merupakan hal sangat penting.
"""

df

x=pd.DataFrame(df[['Open']])
y=pd.DataFrame(df['Adj Close'])

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, shuffle=False)

x_train

y_train

"""Setelah melakukan split kita bisa lanjut ke tahap sandarisasi atau normalisasi agar skala data relatif sama. Disini saya menggunakan StandardScaler."""

numerical_features = ['Open']
scaler = StandardScaler()
scaler.fit(x_train[numerical_features])
x_train[numerical_features] = scaler.transform(x_train.loc[:, numerical_features])
x_train[numerical_features].head()

x_train

y_train

x_test

y_test

"""# Model Development
Seperti yang telah sebutkan, saya akan menggunakan algoritma K-Nearest Neighbor, Random Forest, Boosting Algorithm, dan Neural Network. Pada cell pertama saya akan membuat DataFrame untuk tahap evaluation nanti. Pada cell kedua dan seterusnya saya akan memulai tahap model development. Berdasarkan [04], algoritma KNN secara default menggunakan metrik Euclidean, tetapi terdapat juga metrik yang lain yaitu Minkowski. Untuk menemukan parameter hyperparameter tuning yang tepat saya akan menggunakan teknik GridSearch dari Library Scikit Learn.
"""

# dataframe untuk analisis model
models = pd.DataFrame(index=['train_mse', 'test_mse'], 
                      columns=['KNN', 'KNNTune1', 'RandomForest', 'RandomForestTune1', 'Boosting', 'BoostingTune1', 'NN', 'NNTune1'])
models

"""## K-Nearest Neighbor Models"""

knn = KNeighborsRegressor(n_neighbors=10, metric='euclidean')
knn.fit(x_train, y_train)
y_pred_knn = knn.predict(x_train)

knn_hypt = KNeighborsRegressor()

knn_params = [{'n_neighbors': (range(10, 50)), 'algorithm': ('ball_tree', 'kd_tree', 'brute'), 'metric': ('minkowski', 'euclidean')}]
knngs = GridSearchCV(knn_hypt, knn_params, cv = 15, scoring='neg_mean_squared_error')

knngs.fit(x_train, y_train)

print(knngs.best_params_)
print(knngs.best_score_)

"""Dari diatas kita bisa lihat bahwa paramter-parameter yang paling tepat adalah menggunakan algoritma Brute, metric Minkowski, dan n_neighbors 10."""

knn_hypt = KNeighborsRegressor(algorithm='brute', metric='minkowski', n_neighbors=10)

knn_hypt.fit(x_train, y_train)
y_pred_knn_hypt = knn_hypt.predict(x_train)

"""## Random Forest Models"""

RF = RandomForestRegressor(n_estimators=80, max_depth=20, random_state=None, n_jobs=-1)
RF.fit(x_train, y_train)
 
models.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=RF.predict(x_train), y_true=y_train)

RF_hypt = RandomForestRegressor()

rf_params = [{'n_estimators': (range(100, 135)), 'max_depth': (None, range(1, 30))}]
rfgs = GridSearchCV(RF_hypt, rf_params, cv = 5, scoring='neg_mean_squared_error')

rfgs.fit(x_train, y_train)

print(rfgs.best_params_)
print(rfgs.best_score_)

RF_hypt = RandomForestRegressor(max_depth=None, n_estimators=134)

RF_hypt.fit(x_train, y_train)
models.loc['train_mse','RandomForestTune1'] = mean_squared_error(y_pred=RF_hypt.predict(x_train), y_true=y_train)

"""## Boosting Algorithm"""

boosting = AdaBoostRegressor(n_estimators=50, learning_rate=0.05)                             
boosting.fit(x_train, y_train)
models.loc['train_mse','Boosting'] = mean_squared_error(y_pred=boosting.predict(x_train), y_true=y_train)

boosting_hypt = AdaBoostRegressor()

boost_params = [{'learning_rate': (0.5, 0.05), 'n_estimators': (range(60, 90))}]
btgs = GridSearchCV(boosting_hypt, boost_params, cv = 15, scoring='neg_mean_squared_error')

btgs.fit(x_train, y_train)

print(btgs.best_params_)
print(btgs.best_score_)

boosting_hypt = AdaBoostRegressor(learning_rate=0.5, n_estimators=88)

boosting_hypt.fit(x_train, y_train)
models.loc['train_mse','BoostingTune1'] = mean_squared_error(y_pred=boosting_hypt.predict(x_train), y_true=y_train)

"""## Neural Network"""

model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(1)
  ])

optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(), 
              optimizer=optimizer, 
              metrics=['mse'])

history = model.fit(
    x_train.values, 
    y_train.values, 
    epochs=10, 
    batch_size=64,
    validation_split=0.2)

model_hypt = tf.keras.models.Sequential([
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(1)
  ])

optimizer_hypt = tf.keras.optimizers.SGD(lr=0.01, momentum=0.9)
model_hypt.compile(loss=tf.keras.losses.Huber(), 
              optimizer=optimizer_hypt, 
              metrics=['mse'])

history_hypt = model_hypt.fit(
    x_train.values, 
    y_train.values, 
    epochs=30, 
    batch_size=64,
    validation_split=0.2)

"""# Evaluation
Metrik yang akan digunakan pada tahap evaluasi ada MSE. Rumus:

"""

# Tabel yang akan digunakan
models

x_test.loc[:, numerical_features] = scaler.transform(x_test[numerical_features])
x_test

x_test.describe()

x_train

mse = pd.DataFrame(columns=['train', 'test'], index=['KNN','KNNTune1','RF','RFTune1','Boosting','BoostingTune1','NN','NNTune1'])
model_dict = {'KNN':knn, 'KNNTune1':knn_hypt, 'RF':RF, 'RFTune1':RF_hypt, 'Boosting':boosting, 'BoostingTune1':boosting_hypt, 'NN':model, 'NNTune1':model_hypt}
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(x_train))/1e3 
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(x_test))/1e3
 
mse

mse.drop(['NN', 'NNTune1'], axis=0, inplace=True)
mse

"""Kedua Model Neural Network adalah model yang paling berperforma buruk oleh karena itu kita bisa drop row 'NN' dan 'NNTune1' agar bisa lihat performa model yang lain."""

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=True).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Dengan alasan yang sama saya akan drop row dan Boosting."""

mse.drop(['Boosting'], axis=0, inplace=True)
mse

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=True).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Dari grafik diatas kita bisa simpulkan bahwa model dengan algoritma KNN dan KNN yang sudah di tune adalah model yang memiliki kinerja yang paling bagus. Dalam bidang Training model KNN adalah model yang bagus, tetapi dalam bidang Testing model KNN yang sudah di tune (KNNTune1) lebih unggul. Oleh karena itu kita akan mengambil model KNNTune1 karena lebih unggul dalam bidang testing."""

prediksi = x_test.iloc[:1].copy()
pred_dict = {'y_true':y_test[:1]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)
 
pred_dict

# Rata-rata kenaikan 
raw = y.pct_change(periods=766, fill_method ='ffill').mean()
percentage_raw = raw*100  # output yang dikeluarkan belum berupa persentase
percentage = str(percentage_raw).split()

print('Rata-rata kenaikan harga emas dari periode January 2019-2022 adalah {0}%'.format(percentage[2]))

"""untuk memastikan kita bisa cek sekali lagi dengan cara manual"""

df

nilai_awal = df['Adj Close'][0]
nilai_kenaikan = nilai_awal * (percentage_raw / 100)
nilai_akhir = nilai_awal + nilai_kenaikan

print('nilai harga emas pada akhir dataset (row 766): {}'.format(nilai_akhir))

nilai_kenaikan

# rata-rata persentase kenaikan perhari
raw_h = y.pct_change(periods=1, fill_method ='ffill').mean()
raw_percentage_h = raw_h*100  # output yang dikeluarkan belum berupa persentase
percentage_h = str(raw_percentage_h).split()

print('Rata-rata kenaikan harga emas per hari adalah {0}%'.format(percentage_h[2]))